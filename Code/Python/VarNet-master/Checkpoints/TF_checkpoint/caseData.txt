-------------------------------------------------------------------------------
=============================== VarNet Library ================================
-------------------------------------------------------------------------------
Copyright (c) 2019 Reza Khodayi-mehr - licensed under the MIT License
https://arxiv.org/pdf/1912.07443.pdf

-------------------------------------------------------------------------------
Simulation date: 11/09/2020 - time: 16:33:36

1D time-dependent Advection-Diffusion problem without model-order-reduction.

Boundary condition information:
	type:Dirichlet, Dirichlet

Neural Network architecture:
	type: MLP
	number of inputs: 2
	number of layers: 1
	number of nodes in each layer: [20]
	activation function for each layer: ['sigmoid']
	total number of trainable parameters: 81

Processor information:
	utilized processor: CPU:0

Optimizer information:
	type: Adam stochastic gradient descent algorithm
	learning rate: 0.001

Space-time discretization information:
	spatial domain interior discretization number: [20]
	spatial domain boundary discretization density: None
	temporal discretization number: 100
	number of training points: 2000
	number of training points for BCs: [100, 100]
	total number of BC training points: 200
	number of training points for IC: 20

Sampling scheme for training points: optimal
	fraction of non-uniform points: 0.5
	non-uniform points are added without replacement
	non-uniform points updated once after 20000.0 epochs
	if decrease in 5 consecutive loss values is less than 0.01
	trainable variables are re-initialized after update of training points
Note: since for non-uniform grid the training points and possibly weights are updated
      the loss component plots will not necessarily match total loss plot!

Weighting information:
	requested weights: [10.0, 10.0, 1.0]
	weights on boundary-initial conditions updated after addition of non-uniform points

Stopping criteria:
	maximum number of epochs: 500000
	stopping tolerance: 0.1

==========================================================
Training iterations:

Training weight information:
	boundary condition loss value: 0.0144
	initial condition loss value: 1.1406
	integral loss value: 0.0277
	requested weight on each term: [10.0, 10.0, 1.0]
	corresponding training weights: [863749.6052 863749.6052  86374.9605]

Epoch  1: loss = 1000000.06250
Epoch  2: loss = 996275.68750
Epoch  3: loss = 992604.00000
Epoch  4: loss = 988985.31250
Epoch  5: loss = 985419.56250
Epoch  6: loss = 981904.93750
Epoch  7: loss = 978438.93750
Epoch  8: loss = 975018.81250
Epoch  9: loss = 971641.31250
Epoch 10: loss = 968304.18750
Epoch 11: loss = 965006.12500
Epoch 12: loss = 961746.62500
Epoch 13: loss = 958526.37500
Epoch 14: loss = 955346.62500
Epoch 15: loss = 952207.31250
Epoch 16: loss = 949109.43750
Epoch 17: loss = 946052.62500
Epoch 18: loss = 943036.00000
Epoch 19: loss = 940059.18750
Epoch 20: loss = 937121.56250
Epoch 21: loss = 934223.00000
Epoch 22: loss = 931363.75000
Epoch 23: loss = 928543.93750
Epoch 24: loss = 925764.25000
Epoch 25: loss = 923024.75000
Epoch 26: loss = 920325.75000
Epoch 27: loss = 917667.00000
Epoch 28: loss = 915048.37500
Epoch 29: loss = 912469.50000
Epoch 30: loss = 909930.12500
Epoch 31: loss = 907429.87500
Epoch 32: loss = 904968.93750
Epoch 33: loss = 902546.75000
Epoch 34: loss = 900163.56250
Epoch 35: loss = 897819.00000
Epoch 36: loss = 895512.75000
Epoch 37: loss = 893244.50000
Epoch 38: loss = 891013.93750
Epoch 39: loss = 888820.25000
Epoch 40: loss = 886663.62500
Epoch 41: loss = 884543.50000
Epoch 42: loss = 882459.75000
Epoch 43: loss = 880411.81250
Epoch 44: loss = 878399.68750
Epoch 45: loss = 876422.93750
Epoch 46: loss = 874481.00000
Epoch 47: loss = 872573.68750
Epoch 48: loss = 870700.31250
Epoch 49: loss = 868860.81250
Epoch 50: loss = 867054.75000
Epoch 51: loss = 865281.68750
Epoch 52: loss = 863541.25000
Epoch 53: loss = 861833.25000
Epoch 54: loss = 860157.00000
Epoch 55: loss = 858512.25000
Epoch 56: loss = 856898.50000
Epoch 57: loss = 855315.62500
Epoch 58: loss = 853763.00000
Epoch 59: loss = 852240.18750
Epoch 60: loss = 850746.93750
Epoch 61: loss = 849283.00000
Epoch 62: loss = 847847.81250
Epoch 63: loss = 846441.12500
Epoch 64: loss = 845062.25000
Epoch 65: loss = 843711.25000
Epoch 66: loss = 842387.37500
Epoch 67: loss = 841090.43750
Epoch 68: loss = 839820.12500
Epoch 69: loss = 838575.93750
Epoch 70: loss = 837357.56250
Epoch 71: loss = 836164.50000
Epoch 72: loss = 834996.62500
Epoch 73: loss = 833853.50000
Epoch 74: loss = 832734.68750
Epoch 75: loss = 831639.93750
Epoch 76: loss = 830568.81250
Epoch 77: loss = 829521.00000
Epoch 78: loss = 828496.18750
Epoch 79: loss = 827493.81250
Epoch 80: loss = 826513.93750
Epoch 81: loss = 825555.87500
Epoch 82: loss = 824619.31250
Epoch 83: loss = 823704.31250
Epoch 84: loss = 822810.00000
Epoch 85: loss = 821936.25000
Epoch 86: loss = 821082.87500
Epoch 87: loss = 820249.37500
Epoch 88: loss = 819435.50000
Epoch 89: loss = 818640.87500
Epoch 90: loss = 817865.18750
Epoch 91: loss = 817108.18750
Epoch 92: loss = 816369.50000
Epoch 93: loss = 815648.87500
Epoch 94: loss = 814945.75000
Epoch 95: loss = 814260.12500
Epoch 96: loss = 813591.50000
Epoch 97: loss = 812939.68750
Epoch 98: loss = 812304.12500
Epoch 99: loss = 811685.06250
Epoch 100: loss = 811081.62500
Epoch 200: loss = 790336.81250
Epoch 300: loss = 782216.25000
Epoch 400: loss = 769204.31250
Epoch 500: loss = 748045.43750
Epoch 600: loss = 717359.31250
Epoch 700: loss = 679072.68750
Epoch 800: loss = 638587.18750
Epoch 900: loss = 600671.18750
Epoch 1000: loss = 567925.18750

best model loss: 567925.18750
average iteration time: 0.01482s

